{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325b23a-9a11-4e13-8598-62136b54a421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /Users/marwa/Desktop/Task_C\n",
      "Files: ['train.parquet', 'test.parquet', 'test_sample.parquet', 'validation.parquet', 'sample_submission.csv']\n",
      "Train: (900000, 4)\n",
      "Validation: (200000, 4)\n",
      "Test: (1000, 2)\n",
      "\n",
      "Unique labels: [1 0 2 3]\n",
      "Number of classes: 4\n",
      "\n",
      "Starting hyperparameter search on subsets...\n",
      "\n",
      "Training subset size: 200000\n",
      "Validation subset size: 80000 \n",
      "\n",
      "Testing configuration: 3_4_200k_a5e-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(79532) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (seconds): 335.8\n",
      "Validation Accuracy: 0.7022\n",
      "Validation Macro F1: 0.4915 \n",
      "\n",
      "Testing configuration: 3_4_150k_a1e-4\n",
      "Training time (seconds): 302.4\n",
      "Validation Accuracy: 0.6828\n",
      "Validation Macro F1: 0.4528 \n",
      "\n",
      "Testing configuration: 3_5_150k_a5e-5\n",
      "Training time (seconds): 2616.8\n",
      "Validation Accuracy: 0.7051\n",
      "Validation Macro F1: 0.4968 \n",
      "\n",
      "\n",
      "Hyperparameter search results (sorted):\n",
      "3_5_150k_a5e-5 | F1 = 0.4968 | Accuracy = 0.7051 | ngram = (3, 5) | max_features = 150000 | alpha = 5e-05\n",
      "3_4_200k_a5e-5 | F1 = 0.4915 | Accuracy = 0.7022 | ngram = (3, 4) | max_features = 200000 | alpha = 5e-05\n",
      "3_4_150k_a1e-4 | F1 = 0.4528 | Accuracy = 0.6828 | ngram = (3, 4) | max_features = 150000 | alpha = 0.0001\n",
      "\n",
      "Best configuration: {'name': '3_5_150k_a5e-5', 'ngram': (3, 5), 'max': 150000, 'alpha': 5e-05, 'val_acc': 0.7051125, 'val_f1': 0.49679401556902764, 'fit_time': 2616.839792728424}\n",
      "\n",
      "Training final model on full dataset...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Subtask C — Multi-class Authorship Classification\n",
    "\n",
    "Classes:\n",
    "0: Fully human-written\n",
    "1: Fully machine-generated\n",
    "2: Hybrid (partially human, partially LLM-generated)\n",
    "3: Adversarial LLM output designed to mimic human style\n",
    "\n",
    "Approach:\n",
    "- Character-level TF-IDF\n",
    "- SGDClassifier (logistic regression)\n",
    "- Fast hyperparameter search on a subset of data\n",
    "- Final retraining on full train + validation\n",
    "- Generate submission file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# 1. Set data directory\n",
    "DATA_DIR = \"/Users/marwa/Desktop/Task_C\"\n",
    "\n",
    "print(\"Using data folder:\", DATA_DIR)\n",
    "print(\"Files:\", os.listdir(DATA_DIR))\n",
    "\n",
    "# 2. Load dataset files\n",
    "train = pd.read_parquet(os.path.join(DATA_DIR, \"train.parquet\"))\n",
    "val   = pd.read_parquet(os.path.join(DATA_DIR, \"validation.parquet\"))\n",
    "test  = pd.read_parquet(os.path.join(DATA_DIR, \"test.parquet\"))\n",
    "sample_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Validation:\", val.shape)\n",
    "print(\"Test:\", test.shape)\n",
    "\n",
    "\n",
    "# 3. Inspect label distribution\n",
    "print(\"\\nUnique labels:\", train[\"label\"].unique())\n",
    "print(\"Number of classes:\", train[\"label\"].nunique())\n",
    "\n",
    "# 4. Build input text (language + code)\n",
    "def make_text(df: pd.DataFrame) -> pd.Series:\n",
    "    code = df[\"code\"].fillna(\"\")\n",
    "    if \"language\" in df.columns:\n",
    "        lang = df[\"language\"].fillna(\"\")\n",
    "        return lang + \" \" + code\n",
    "    return code\n",
    "\n",
    "X_train = make_text(train)\n",
    "X_val   = make_text(val)\n",
    "X_test  = make_text(test)\n",
    "\n",
    "y_train = train[\"label\"].values\n",
    "y_val   = val[\"label\"].values\n",
    "\n",
    "# 5. Model builder: TF-IDF + SGDClassifier\n",
    "def build_model(\n",
    "    ngram=(3, 4),\n",
    "    max_features=200_000,\n",
    "    alpha=5e-5,\n",
    "):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=ngram,\n",
    "            max_features=max_features,\n",
    "            min_df=10,\n",
    "        )),\n",
    "        (\"clf\", SGDClassifier(\n",
    "            loss=\"log_loss\",\n",
    "            alpha=alpha,\n",
    "            max_iter=10_000,\n",
    "            tol=1e-3,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "# ================================================================\n",
    "# 6. Fast hyperparameter search using subsets of data\n",
    "# ================================================================\n",
    "\n",
    "# Subset sizes for faster experimentation\n",
    "N_TRAIN_HP = 200_000\n",
    "N_VAL_HP   = 80_000\n",
    "\n",
    "X_train_hp = X_train.iloc[:N_TRAIN_HP]\n",
    "y_train_hp = y_train[:N_TRAIN_HP]\n",
    "X_val_hp   = X_val.iloc[:N_VAL_HP]\n",
    "y_val_hp   = y_val[:N_VAL_HP]\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"3_4_200k_a5e-5\",  \"ngram\": (3, 4), \"max\": 200_000, \"alpha\": 5e-5},\n",
    "    {\"name\": \"3_4_150k_a1e-4\",  \"ngram\": (3, 4), \"max\": 150_000, \"alpha\": 1e-4},\n",
    "    {\"name\": \"3_5_150k_a5e-5\",  \"ngram\": (3, 5), \"max\": 150_000, \"alpha\": 5e-5},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting hyperparameter search on subsets...\\n\")\n",
    "print(\"Training subset size:\", len(X_train_hp))\n",
    "print(\"Validation subset size:\", len(X_val_hp), \"\\n\")\n",
    "\n",
    "for cfg in configs:\n",
    "    print(\"Testing configuration:\", cfg[\"name\"])\n",
    "    t0 = time.time()\n",
    "\n",
    "    model = build_model(\n",
    "        ngram=cfg[\"ngram\"],\n",
    "        max_features=cfg[\"max\"],\n",
    "        alpha=cfg[\"alpha\"],\n",
    "    )\n",
    "    model.fit(X_train_hp, y_train_hp)\n",
    "\n",
    "    fit_time = time.time() - t0\n",
    "    print(\"Training time (seconds):\", round(fit_time, 1))\n",
    "\n",
    "    val_pred = model.predict(X_val_hp)\n",
    "    acc = accuracy_score(y_val_hp, val_pred)\n",
    "    macro_f1 = f1_score(y_val_hp, val_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Validation Accuracy:\", round(acc, 4))\n",
    "    print(\"Validation Macro F1:\", round(macro_f1, 4), \"\\n\")\n",
    "\n",
    "    cfg[\"val_acc\"] = acc\n",
    "    cfg[\"val_f1\"]  = macro_f1\n",
    "    cfg[\"fit_time\"] = fit_time\n",
    "    results.append(cfg)\n",
    "\n",
    "# Sort configurations by Macro F1\n",
    "results = sorted(results, key=lambda d: d[\"val_f1\"], reverse=True)\n",
    "best = results[0]\n",
    "\n",
    "print(\"\\nHyperparameter search results (sorted):\")\n",
    "for r in results:\n",
    "    print(\n",
    "        r['name'],\n",
    "        \"| F1 =\", round(r['val_f1'], 4),\n",
    "        \"| Accuracy =\", round(r['val_acc'], 4),\n",
    "        \"| ngram =\", r['ngram'],\n",
    "        \"| max_features =\", r['max'],\n",
    "        \"| alpha =\", r['alpha']\n",
    "    )\n",
    "\n",
    "print(\"\\nBest configuration:\", best)\n",
    "\n",
    "# 7. Final training on full train + validation data\n",
    "print(\"\\nTraining final model on full dataset...\\n\")\n",
    "\n",
    "X_full = pd.concat([X_train, X_val], axis=0)\n",
    "y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "final_model = build_model(\n",
    "    ngram=best[\"ngram\"],\n",
    "    max_features=best[\"max\"],\n",
    "    alpha=best[\"alpha\"],\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "final_model.fit(X_full, y_full)\n",
    "full_time = (time.time() - t0) / 60\n",
    "print(\"Final training time (minutes):\", round(full_time, 1))\n",
    "\n",
    "model_file = f\"subtaskC_model_{best['name']}.joblib\"\n",
    "model_path = os.path.join(DATA_DIR, model_file)\n",
    "joblib.dump(final_model, model_path)\n",
    "print(\"Model saved:\", model_path)\n",
    "\n",
    "# 8. Generate submission file\n",
    "print(\"\\nGenerating submission file...\")\n",
    "\n",
    "test_pred = final_model.predict(X_test)\n",
    "sample_sub[\"label\"] = test_pred\n",
    "\n",
    "sub_name = f\"submission_subtaskC_{best['name']}.csv\"\n",
    "sub_path = os.path.join(DATA_DIR, sub_name)\n",
    "sample_sub.to_csv(sub_path, index=False)\n",
    "\n",
    "print(\"Submission file created:\", sub_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74a3861-f514-41ce-b260-4725f6263113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "\n",
    "print(os.path.exists(\"model_subtaskC.joblib\"))\n",
    "if os.path.exists(\"model_subtaskC.joblib\"):\n",
    "    print(\"Size (MB):\", os.path.getsize(\"model_subtaskC.joblib\") / (1024**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba6c298-3dc1-4a86-8098-9a3c2f0b424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CONFIG = {\n",
    "    \"ngram\": (3, 5),\n",
    "    \"max_features\": 120000,  # save because of memorey \n",
    "    \"alpha\": 5e-5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9268879-92e6-4319-b222-0d8980081613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /Users/marwa/Desktop/Task_C\n",
      "Files in folder: ['train.parquet', 'test.parquet', 'test_sample.parquet', 'validation.parquet', 'sample_submission.csv']\n",
      "Loading train and validation data...\n",
      "Train shape: (900000, 4)\n",
      "Validation shape: (200000, 4)\n",
      "Full training data shape: (1100000, 4)\n",
      "Fitting TF-IDF on full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2043: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float32 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import joblib\n",
    "\n",
    "# 1) Folder path\n",
    "DATA_DIR = \"/Users/marwa/Desktop/Task_C\"\n",
    "\n",
    "print(\"Using data folder:\", DATA_DIR)\n",
    "print(\"Files in folder:\", os.listdir(DATA_DIR))\n",
    "\n",
    "# 2) Load train + validation\n",
    "print(\"Loading train and validation data...\")\n",
    "train = pd.read_parquet(os.path.join(DATA_DIR, \"train.parquet\"))\n",
    "val = pd.read_parquet(os.path.join(DATA_DIR, \"validation.parquet\"))\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Validation shape:\", val.shape)\n",
    "\n",
    "# Combine train + validation into one DataFrame\n",
    "full_df = pd.concat([train, val], axis=0).reset_index(drop=True)\n",
    "print(\"Full training data shape:\", full_df.shape)\n",
    "\n",
    "# Features (code text) and labels\n",
    "X_text = full_df[\"code\"]\n",
    "y = full_df[\"label\"]\n",
    "\n",
    "# 3) TF-IDF vectorizer\n",
    "\n",
    "# Char-level TF-IDF with n-grams 3 to 5 and 150k features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=150000,\n",
    "    dtype=\"float32\",  # will be converted to float64 internally\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF on full dataset...\")\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# 4) Classifier (SGD with log-loss)\n",
    "clf = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    alpha=5e-5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Training classifier on full dataset...\")\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 5) Save model (vectorizer + classifier)\n",
    "bundle = {\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"classifier\": clf,\n",
    "}\n",
    "\n",
    "MODEL_PATH = os.path.join(DATA_DIR, \"model_subtaskC_best.joblib\")\n",
    "joblib.dump(bundle, MODEL_PATH)\n",
    "\n",
    "print(\"✅ Done. Model saved to:\", MODEL_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
