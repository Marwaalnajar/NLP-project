{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fcef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/shanuaimi/Desktop/Natural Language Processing /Task_B\n",
      "Files in this folder: ['Untitled1.ipynb', 'train.parquet', '.DS_Store', 'test.parquet', 'test_sample.parquet', 'Untitled.ipynb', 'subtaskB_sgd_char_3_4_150k_alpha5e-5.joblib', 'submission_sgd_char_3_4_150k_alpha5e-5.csv', 'submission.csv', 'submission1.csv', 'subtaskB_model.joblib', 'validation.parquet', 'second trial.ipynb', '.ipynb_checkpoints', 'Fourth run.ipynb', 'sample_submission.csv']\n",
      "Train size      : 500000\n",
      "Validation size : 100000\n",
      "Test size       : 1000\n",
      "Test_sample size: 1000\n",
      "\n",
      "================ HYPERPARAMETER SEARCH (light) ================\n",
      "\n",
      "Using a subsample of 200000 for hyperparameter search.\n",
      "Training config: char_3_5_120k_alpha5e-5\n",
      "  Validation Accuracy : 0.8710\n",
      "  Validation Macro F1 : 0.2725\n",
      "\n",
      "Training config: char_3_4_120k_alpha5e-5\n",
      "  Validation Accuracy : 0.8699\n",
      "  Validation Macro F1 : 0.2678\n",
      "\n",
      "Training config: char_3_5_100k_alpha1e-4\n",
      "  Validation Accuracy : 0.8694\n",
      "  Validation Macro F1 : 0.2435\n",
      "\n",
      "Training config: char_3_4_100k_alpha1e-4\n",
      "  Validation Accuracy : 0.8683\n",
      "  Validation Macro F1 : 0.2365\n",
      "\n",
      "===== Validation Results (sorted by Macro F1) =====\n",
      "char_3_5_120k_alpha5e-5: Macro F1=0.2725, Accuracy=0.8710, ngram=(3, 5), max_features=120000, alpha=5e-05\n",
      "char_3_4_120k_alpha5e-5: Macro F1=0.2678, Accuracy=0.8699, ngram=(3, 4), max_features=120000, alpha=5e-05\n",
      "char_3_5_100k_alpha1e-4: Macro F1=0.2435, Accuracy=0.8694, ngram=(3, 5), max_features=100000, alpha=0.0001\n",
      "char_3_4_100k_alpha1e-4: Macro F1=0.2365, Accuracy=0.8683, ngram=(3, 4), max_features=100000, alpha=0.0001\n",
      "\n",
      "Best config selected: char_3_5_120k_alpha5e-5\n",
      "\n",
      "\n",
      "================ TRAINING FINAL MODEL ON TRAIN+VAL ================\n",
      "\n",
      "Final model saved as: subtaskB_sgd_balanced_model.joblib\n",
      "\n",
      "================ EVALUATION ON TEST_SAMPLE (LOCAL ONLY) ================\n",
      "\n",
      "Test_sample Accuracy : 0.5450\n",
      "Test_sample Macro F1 : 0.2188\n",
      "\n",
      "Classification report on test_sample:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.95      0.80       474\n",
      "           1       0.19      0.14      0.16        21\n",
      "           2       0.14      0.01      0.03        73\n",
      "           3       0.47      0.33      0.39        21\n",
      "           4       0.05      0.10      0.06        10\n",
      "           5       0.09      0.22      0.12        36\n",
      "           6       0.27      0.24      0.25        54\n",
      "           7       0.00      0.00      0.00        61\n",
      "           8       0.08      0.17      0.11        18\n",
      "           9       0.10      0.11      0.10        18\n",
      "          10       0.68      0.26      0.38       214\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.25      0.23      0.22      1000\n",
      "weighted avg       0.52      0.55      0.50      1000\n",
      "\n",
      "\n",
      "================ GENERATING SUBMISSION FILE ================\n",
      "\n",
      "Submission file created: submission_sgd_balanced_new.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Subtask B - New Submission\n",
    "==========================\n",
    "\n",
    "- Model: TF-IDF (char n-grams) + SGDClassifier (log-loss, linear classifier)\n",
    "- Goal: Improve generalization & macro-F1 using:\n",
    "    * class_weight=\"balanced\"\n",
    "    * log-loss (probabilistic) instead of hinge\n",
    "    * slightly tuned n-gram range and feature size\n",
    "\n",
    "This script:\n",
    "1. Loads the SemEval-2026 Task 13 Subtask B data.\n",
    "2. Builds TF-IDF features from language + code.\n",
    "3. Runs a small hyperparameter search on a *subset* of train for speed.\n",
    "4. Trains a final model on full train + validation using the best config.\n",
    "5. Evaluates on test_sample.parquet (local only).\n",
    "6. Generates a submission CSV for Kaggle.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Paths & basic setup\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def set_working_directory():\n",
    "    \"\"\"\n",
    "    Set the working directory to the Task_B folder on your MacBook.\n",
    "    Change the path below if your folder is in a different location.\n",
    "    \"\"\"\n",
    "    task_b_path = \"/Users/shanuaimi/Desktop/Natural Language Processing /Task_B\"\n",
    "    os.chdir(task_b_path)\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    print(\"Files in this folder:\", os.listdir())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Data loading & text construction\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load all parquet files for Subtask B:\n",
    "    - train.parquet\n",
    "    - validation.parquet\n",
    "    - test.parquet\n",
    "    - test_sample.parquet\n",
    "    Returns: train_df, val_df, test_df, test_sample_df\n",
    "    \"\"\"\n",
    "    train_df = pd.read_parquet(\"train.parquet\")\n",
    "    val_df = pd.read_parquet(\"validation.parquet\")\n",
    "    test_df = pd.read_parquet(\"test.parquet\")\n",
    "    test_sample_df = pd.read_parquet(\"test_sample.parquet\")\n",
    "\n",
    "    # Ensure labels are integers\n",
    "    train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "    val_df[\"label\"] = val_df[\"label\"].astype(int)\n",
    "    test_sample_df[\"label\"] = test_sample_df[\"label\"].astype(int)\n",
    "\n",
    "    print(f\"Train size      : {len(train_df)}\")\n",
    "    print(f\"Validation size : {len(val_df)}\")\n",
    "    print(f\"Test size       : {len(test_df)}\")\n",
    "    print(f\"Test_sample size: {len(test_sample_df)}\")\n",
    "\n",
    "    return train_df, val_df, test_df, test_sample_df\n",
    "\n",
    "\n",
    "def build_text_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Construct the text input for TF-IDF from language + code.\n",
    "\n",
    "    We keep preprocessing minimal on purpose:\n",
    "    - language: gives a strong signal (Python, C++, Java, etc.)\n",
    "    - code: raw source code, no stripping or cleaning\n",
    "    \"\"\"\n",
    "    code = df[\"code\"].fillna(\"\")\n",
    "    if \"language\" in df.columns:\n",
    "        lang = df[\"language\"].fillna(\"\")\n",
    "        text = (lang + \" \" + code).astype(str)\n",
    "    else:\n",
    "        text = code.astype(str)\n",
    "    return text\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Model building\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def build_pipeline(ngram_range=(3, 5), max_features=150_000, alpha=5e-5):\n",
    "    \"\"\"\n",
    "    Build a TF-IDF + SGDClassifier pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_range : tuple\n",
    "        Character n-gram range, e.g. (3, 4) or (3, 5).\n",
    "    max_features : int\n",
    "        Maximum number of TF-IDF features to keep.\n",
    "    alpha : float\n",
    "        Regularization strength for SGDClassifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Pipeline\n",
    "        Sklearn pipeline with TF-IDF + SGD classifier.\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=5,                 # drop very rare n-grams to reduce noise\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"log_loss\",          # probabilistic linear classifier\n",
    "        penalty=\"l2\",\n",
    "        alpha=alpha,\n",
    "        max_iter=20,              # keep iterations moderate for speed\n",
    "        tol=1e-3,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",  # help with label imbalance\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"tfidf\", tfidf),\n",
    "        (\"clf\", clf),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Hyperparameter search (lightweight)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def hyperparameter_search(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    max_train_samples=200_000\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a small hyperparameter search on a subset of the training data\n",
    "    to keep runtime manageable on a MacBook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train_full : pd.Series\n",
    "        Full training text data.\n",
    "    y_train_full : np.ndarray\n",
    "        Full training labels.\n",
    "    X_val : pd.Series\n",
    "        Validation text data.\n",
    "    y_val : np.ndarray\n",
    "        Validation labels.\n",
    "    max_train_samples : int\n",
    "        Maximum number of training samples to use during search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_cfg : dict\n",
    "        Dictionary containing the best configuration (ngram, max_features, alpha).\n",
    "    \"\"\"\n",
    "    print(\"\\n================ HYPERPARAMETER SEARCH (light) ================\\n\")\n",
    "\n",
    "    # Optionally subsample training data for speed\n",
    "    if len(X_train_full) > max_train_samples:\n",
    "        X_train_sub, y_train_sub = shuffle(\n",
    "            X_train_full,\n",
    "            y_train_full,\n",
    "            random_state=42\n",
    "        )\n",
    "        X_train_sub = X_train_sub.iloc[:max_train_samples]\n",
    "        y_train_sub = y_train_sub[:max_train_samples]\n",
    "        print(f\"Using a subsample of {max_train_samples} for hyperparameter search.\")\n",
    "    else:\n",
    "        X_train_sub, y_train_sub = X_train_full, y_train_full\n",
    "        print(\"Using full training set for hyperparameter search (size <= max_train_samples).\")\n",
    "\n",
    "    # Candidates: different n-grams and feature sizes\n",
    "    configs = [\n",
    "        {\"name\": \"char_3_5_120k_alpha5e-5\", \"ngram\": (3, 5), \"max_features\": 120_000, \"alpha\": 5e-5},\n",
    "        {\"name\": \"char_3_4_120k_alpha5e-5\", \"ngram\": (3, 4), \"max_features\": 120_000, \"alpha\": 5e-5},\n",
    "        {\"name\": \"char_3_5_100k_alpha1e-4\", \"ngram\": (3, 5), \"max_features\": 100_000, \"alpha\": 1e-4},\n",
    "        {\"name\": \"char_3_4_100k_alpha1e-4\", \"ngram\": (3, 4), \"max_features\": 100_000, \"alpha\": 1e-4},\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for cfg in configs:\n",
    "        print(f\"Training config: {cfg['name']}\")\n",
    "        model = build_pipeline(\n",
    "            ngram_range=cfg[\"ngram\"],\n",
    "            max_features=cfg[\"max_features\"],\n",
    "            alpha=cfg[\"alpha\"]\n",
    "        )\n",
    "        model.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "        val_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, val_pred)\n",
    "        macro_f1 = f1_score(y_val, val_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"  Validation Accuracy : {acc:.4f}\")\n",
    "        print(f\"  Validation Macro F1 : {macro_f1:.4f}\\n\")\n",
    "\n",
    "        results.append((cfg, macro_f1, acc))\n",
    "\n",
    "    print(\"===== Validation Results (sorted by Macro F1) =====\")\n",
    "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    for cfg, macro_f1, acc in results_sorted:\n",
    "        print(f\"{cfg['name']}: Macro F1={macro_f1:.4f}, Accuracy={acc:.4f}, \"\n",
    "              f\"ngram={cfg['ngram']}, max_features={cfg['max_features']}, alpha={cfg['alpha']}\")\n",
    "\n",
    "    best_cfg = results_sorted[0][0]\n",
    "    print(f\"\\nBest config selected: {best_cfg['name']}\\n\")\n",
    "    return best_cfg\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Train final model & evaluate\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def train_final_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    best_cfg,\n",
    "    model_path=\"subtaskB_sgd_balanced_model.joblib\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a final model on train + validation using the best hyperparameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.Series\n",
    "        Training text data.\n",
    "    y_train : np.ndarray\n",
    "        Training labels.\n",
    "    X_val : pd.Series\n",
    "        Validation text data.\n",
    "    y_val : np.ndarray\n",
    "        Validation labels.\n",
    "    best_cfg : dict\n",
    "        Best hyperparameters chosen from hyperparameter_search.\n",
    "    model_path : str\n",
    "        File name for saving the trained model.\n",
    "    \"\"\"\n",
    "    print(\"\\n================ TRAINING FINAL MODEL ON TRAIN+VAL ================\\n\")\n",
    "\n",
    "    full_X = pd.concat([X_train, X_val], axis=0)\n",
    "    full_y = np.concatenate([y_train, y_val])\n",
    "\n",
    "    model = build_pipeline(\n",
    "        ngram_range=best_cfg[\"ngram\"],\n",
    "        max_features=best_cfg[\"max_features\"],\n",
    "        alpha=best_cfg[\"alpha\"]\n",
    "    )\n",
    "    model.fit(full_X, full_y)\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Final model saved as: {model_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_local(model, X_test_sample, y_test_sample):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test_sample.parquet (local only).\n",
    "    This does NOT affect the Kaggle leaderboard; it's for sanity check.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Pipeline\n",
    "        Trained model.\n",
    "    X_test_sample : pd.Series\n",
    "        Text data from test_sample.parquet.\n",
    "    y_test_sample : np.ndarray\n",
    "        Labels from test_sample.parquet.\n",
    "    \"\"\"\n",
    "    print(\"\\n================ EVALUATION ON TEST_SAMPLE (LOCAL ONLY) ================\\n\")\n",
    "\n",
    "    ts_pred = model.predict(X_test_sample)\n",
    "    acc = accuracy_score(y_test_sample, ts_pred)\n",
    "    macro_f1 = f1_score(y_test_sample, ts_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Test_sample Accuracy : {acc:.4f}\")\n",
    "    print(f\"Test_sample Macro F1 : {macro_f1:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification report on test_sample:\\n\")\n",
    "    print(classification_report(y_test_sample, ts_pred))\n",
    "\n",
    "\n",
    "def generate_submission(model, X_test, submission_name=\"submission_sgd_balanced_new.csv\"):\n",
    "    \"\"\"\n",
    "    Generate a Kaggle submission file for Subtask B.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Pipeline\n",
    "        Trained model.\n",
    "    X_test : pd.Series\n",
    "        Test text data from test.parquet (no labels).\n",
    "    submission_name : str\n",
    "        Output CSV file name.\n",
    "    \"\"\"\n",
    "    print(\"\\n================ GENERATING SUBMISSION FILE ================\\n\")\n",
    "\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    sample_sub = pd.read_csv(\"sample_submission.csv\")\n",
    "    if len(sample_sub) != len(test_pred):\n",
    "        raise ValueError(\n",
    "            f\"sample_submission length ({len(sample_sub)}) \"\n",
    "            f\"does not match test predictions ({len(test_pred)})!\"\n",
    "        )\n",
    "\n",
    "    sample_sub[\"label\"] = test_pred.astype(int)\n",
    "    sample_sub.to_csv(submission_name, index=False)\n",
    "\n",
    "    print(f\"Submission file created: {submission_name}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Main execution\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_working_directory()\n",
    "\n",
    "    # Load data\n",
    "    train_df, val_df, test_df, test_sample_df = load_datasets()\n",
    "\n",
    "    # Build text inputs\n",
    "    X_train = build_text_column(train_df)\n",
    "    X_val = build_text_column(val_df)\n",
    "    X_test = build_text_column(test_df)\n",
    "    X_test_sample = build_text_column(test_sample_df)\n",
    "\n",
    "    y_train = train_df[\"label\"].values\n",
    "    y_val = val_df[\"label\"].values\n",
    "    y_test_sample = test_sample_df[\"label\"].values\n",
    "\n",
    "    # Hyperparameter search (light, on subset for speed)\n",
    "    best_cfg = hyperparameter_search(\n",
    "        X_train_full=X_train,\n",
    "        y_train_full=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        max_train_samples=200_000  # you can lower this if your Mac is slow\n",
    "    )\n",
    "\n",
    "    # Train final model\n",
    "    final_model = train_final_model(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        best_cfg=best_cfg,\n",
    "        model_path=\"subtaskB_sgd_balanced_model.joblib\"\n",
    "    )\n",
    "\n",
    "    # Local evaluation on test_sample.parquet\n",
    "    evaluate_local(\n",
    "        model=final_model,\n",
    "        X_test_sample=X_test_sample,\n",
    "        y_test_sample=y_test_sample\n",
    "    )\n",
    "\n",
    "    # Generate submission file for Kaggle\n",
    "    generate_submission(\n",
    "        model=final_model,\n",
    "        X_test=X_test,\n",
    "        submission_name=\"submission_sgd_balanced_new.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4beae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
